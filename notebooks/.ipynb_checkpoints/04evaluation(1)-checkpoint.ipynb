{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9856798,"sourceType":"datasetVersion","datasetId":6048877}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ff698db4c87d7582","cell_type":"markdown","source":"# Ejercicio 04: Evaluación de un Sistema de Recuperación de Información\n\n\n\nEl objetivo de este ejercicio es evaluar la efectividad de un sistema de recuperación de información utilizando métricas como *precisión*, *recall*, *F1-score*, *Mean Average Precision (MAP)* y *Normalized Discounted Cumulative Gain (nDCG)*.\n\n\n\nSeguirás los siguientes pasos:","metadata":{}},{"id":"a79fed129105d246","cell_type":"markdown","source":"Descripción del Ejercicio\n\n\n\n1. Proporcionar un Conjunto de Datos:\n\n    * Corpus de Documentos: Utiliza el corpus del ejercicio anterior o un nuevo conjunto de documentos.\n\n    * Consultas: Define un conjunto de consultas específicas.\n\n    * Juicios de Relevancia: Proporciona una lista de qué documentos son relevantes para cada consulta.\n\n\n\n2. Calcular Resultados de Búsqueda:\n\n    * Obten los resultados ordenados de dos sistemas de recuperación para cada consulta.\n\n\n\n3. Calcular las Métricas de Evaluación:\n\n    * Calcular las siguientes métricas para cada sistema y consulta:\n\n        * Precisión en el top-k (Prec@k)\n\n        * Recall\n\n        * F1-score\n\n        * Mean Average Precision (MAP)\n\n        * nDCG\n\n\n\n4. Análisis y Comparación:\n\n    * Comparar los resultados de los dos sistemas utilizando las métricas calculadas.\n\n    * Discutir cuál sistema es más efectivo y por qué.","metadata":{}},{"id":"60c1eafa-ac48-4a61-bdfb-a8ef32df65d4","cell_type":"code","source":"import xml.etree.ElementTree as ET\nimport numpy as np\n\n# Paso 1: Proporcionar un Conjunto de Datos\n# Leer y parsear el archivo XML para obtener el corpus de documentos\ndef parse_corpus(xml_file):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    corpus = {}\n    for doc in root.findall('document'):\n        doc_id = int(doc.get('id'))\n        title = doc.find('title').text\n        keywords = doc.find('keywords').text\n        author = doc.find('author').text\n        date = doc.find('date').text\n        keyword_set = process_text(keywords)\n        corpus[doc_id] = {\n            'title': title,\n            'keywords': keyword_set,\n            'author': author,\n            'date': date\n        }\n    return corpus\n\n# Función para procesar el texto y extraer palabras clave, útil para manejar el corpus y las consultas\ndef process_text(text):\n    text = text.lower()\n    import re\n    text = re.sub(r'[^a-záéíóúñü]+', ' ', text)\n    tokens = text.strip().split()\n    return set(tokens)\n\n# Leer el corpus de documentos\ncorpus = parse_corpus('/kaggle/input/03ranking-corpus-xml/03ranking_corpus.xml')\n\n# Consultas: Definir un conjunto de consultas específicas\nqueries = {\n    'query1': process_text(\"salud mental en estudiantes\"),\n    'query2': process_text(\"tecnología médica preventiva\"),\n}\n\n# Juicios de Relevancia: Proporcionar una lista de qué documentos son relevantes para cada consulta\nrelevance_judgments = {\n    'query1': {13, 14, 8, 7, 12},\n    'query2': {1, 10, 20},\n}\n\n# Paso 2: Calcular Resultados de Búsqueda\n# Simular los resultados de búsqueda de dos sistemas de recuperación para cada consulta\nsystem_1_results = {\n    'query1': [13, 7, 12, 4, 8],\n    'query2': [1, 10, 3, 20, 11],\n}\n\nsystem_2_results = {\n    'query1': [14, 8, 13, 7, 6],\n    'query2': [10, 1, 20, 22, 30],\n}\n\n# Paso 3: Calcular las Métricas de Evaluación\n# Definir funciones para calcular Precisión en el top-k (Prec@k), Recall, F1-score, MAP y nDCG\n\n# Precisión en el top-k (Prec@k)\ndef precision_at_k(results, relevant_set, k):\n    retrieved_set = set(results[:k])\n    return len(retrieved_set & relevant_set) / k\n\n# Recall\ndef recall(results, relevant_set):\n    retrieved_set = set(results)\n    return len(retrieved_set & relevant_set) / len(relevant_set)\n\n# F1-score\ndef f1_score_custom(precision, recall):\n    if precision + recall == 0:\n        return 0\n    return 2 * (precision * recall) / (precision + recall)\n\n# Mean Average Precision (MAP)\ndef average_precision(results, relevant_set):\n    relevant_count = 0\n    sum_precision = 0.0\n    for i, doc_id in enumerate(results, start=1):\n        if doc_id in relevant_set:\n            relevant_count += 1\n            sum_precision += relevant_count / i\n    return sum_precision / len(relevant_set) if relevant_set else 0\n\ndef mean_average_precision(system_results, queries, relevance_judgments):\n    avg_precision_total = 0\n    for query, results in system_results.items():\n        relevant_set = relevance_judgments.get(query, set())\n        avg_precision_total += average_precision(results, relevant_set)\n    return avg_precision_total / len(queries)\n\n# Normalized Discounted Cumulative Gain (nDCG)\ndef dcg_at_k(results, relevant_set, k):\n    dcg = 0.0\n    for i in range(min(k, len(results))):\n        if results[i] in relevant_set:\n            dcg += 1 / np.log2(i + 2)\n    return dcg\n\ndef ndcg_at_k(results, relevant_set, k):\n    dcg_max = dcg_at_k(list(relevant_set), relevant_set, k)\n    if not dcg_max:\n        return 0\n    return dcg_at_k(results, relevant_set, k) / dcg_max\n\n# Paso 4: Análisis y Comparación\n# Comparar los resultados de los dos sistemas utilizando las métricas calculadas y calcular promedios\n\nfor system_name, system_results in [('System 1', system_1_results), ('System 2', system_2_results)]:\n    print(f\"\\n--- Metrics for {system_name} ---\")\n    \n    # Acumuladores de métricas para calcular el promedio al final\n    total_prec_k = total_recall = total_f1 = total_map = total_ndcg = 0\n    num_queries = len(queries)\n\n    for query, relevant_set in relevance_judgments.items():\n        results = system_results[query]\n        \n        # Calcular cada métrica para la consulta actual\n        prec_k = precision_at_k(results, relevant_set, k=5)\n        rec = recall(results, relevant_set)\n        f1 = f1_score_custom(prec_k, rec)\n        map_score = mean_average_precision({query: results}, {query: relevant_set}, relevance_judgments)\n        ndcg = ndcg_at_k(results, relevant_set, k=5)\n        \n        # Sumar las métricas para calcular promedios más adelante\n        total_prec_k += prec_k\n        total_recall += rec\n        total_f1 += f1\n        total_map += map_score\n        total_ndcg += ndcg\n\n        # Mostrar métricas por consulta para cada sistema\n        print(f\"\\nResults for {query}:\")\n        print(f\"  Prec@5: {prec_k:.4f}\")\n        print(f\"  Recall: {rec:.4f}\")\n        print(f\"  F1-Score: {f1:.4f}\")\n        print(f\"  MAP: {map_score:.4f}\")\n        print(f\"  nDCG@5: {ndcg:.4f}\")\n\n    # Calcular promedios de métricas para cada sistema\n    avg_prec_k = total_prec_k / num_queries\n    avg_recall = total_recall / num_queries\n    avg_f1 = total_f1 / num_queries\n    avg_map = total_map / num_queries\n    avg_ndcg = total_ndcg / num_queries\n\n    # Mostrar promedios finales para el sistema actual\n    print(f\"\\nAverage Metrics for {system_name}:\")\n    print(f\"  Avg Prec@5: {avg_prec_k:.4f}\")\n    print(f\"  Avg Recall: {avg_recall:.4f}\")\n    print(f\"  Avg F1-Score: {avg_f1:.4f}\")\n    print(f\"  Avg MAP: {avg_map:.4f}\")\n    print(f\"  Avg nDCG@5: {avg_ndcg:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T22:20:21.624099Z","iopub.execute_input":"2024-11-09T22:20:21.624548Z","iopub.status.idle":"2024-11-09T22:20:21.661799Z","shell.execute_reply.started":"2024-11-09T22:20:21.624505Z","shell.execute_reply":"2024-11-09T22:20:21.660530Z"}},"outputs":[{"name":"stdout","text":"\n--- Metrics for System 1 ---\n\nResults for query1:\n  Prec@5: 0.8000\n  Recall: 0.8000\n  F1-Score: 0.8000\n  MAP: 0.7600\n  nDCG@5: 0.8539\n\nResults for query2:\n  Prec@5: 0.6000\n  Recall: 1.0000\n  F1-Score: 0.7500\n  MAP: 0.9167\n  nDCG@5: 0.9675\n\nAverage Metrics for System 1:\n  Avg Prec@5: 0.7000\n  Avg Recall: 0.9000\n  Avg F1-Score: 0.7750\n  Avg MAP: 0.8383\n  Avg nDCG@5: 0.9107\n\n--- Metrics for System 2 ---\n\nResults for query1:\n  Prec@5: 0.8000\n  Recall: 0.8000\n  F1-Score: 0.8000\n  MAP: 0.8000\n  nDCG@5: 0.8688\n\nResults for query2:\n  Prec@5: 0.6000\n  Recall: 1.0000\n  F1-Score: 0.7500\n  MAP: 1.0000\n  nDCG@5: 1.0000\n\nAverage Metrics for System 2:\n  Avg Prec@5: 0.7000\n  Avg Recall: 0.9000\n  Avg F1-Score: 0.7750\n  Avg MAP: 0.9000\n  Avg nDCG@5: 0.9344\n","output_type":"stream"}],"execution_count":7},{"id":"cedfa928-47b7-4ae1-a26e-a7e93d44814d","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}