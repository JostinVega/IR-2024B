{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de29663-abb8-4a19-8b56-cc833983be62",
   "metadata": {},
   "source": [
    "# Proyecto Bimestral: Sistema de Recuperación de Información basado en Reuters-21578\n",
    "\n",
    "## 1. Introducci´on\n",
    "El objetivo de este proyecto es diseñar, construir, programar y desplegar un Sistema de Recuperación de Información (SRI) utilizando el corpus Reuters-21578. El proyecto se dividirá en varias fases,\n",
    "que se describen a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c26c5c-0347-4606-ae07-dafbc743c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf1e8b-ed2e-4081-a8b4-50a11c123303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca0452-e31e-4e91-9437-47bb1139bf6b",
   "metadata": {},
   "source": [
    "## 2. Fases del Proyecto\n",
    "### 2.1. Adquisici´on de Datos\n",
    "- Objetivo: Obtener y preparar el corpus Reuters-21578.\n",
    "- Tareas:\n",
    "    - Descargar el corpus Reuters-21578.\n",
    "    - Descomprimir y organizar los archivos.\n",
    "    - Documentar el proceso de adquisici´on de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e483f30-e93d-457c-8415-ec2f72fa6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar y descomprimir el corpus Reuters-21578\n",
    "def extract_reuters_data(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"Datos descomprimidos en:\", extract_to)\n",
    "\n",
    "# Ruta al archivo zip descargado y carpeta de destino\n",
    "zip_path = r\"..\\data\\reuters.zip\"\n",
    "extract_to = r\"..\\data\"\n",
    "extract_reuters_data(zip_path, extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a32c03-9da8-432d-815f-1f498ac85899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_extension_to_txt(folder_path):\n",
    "    \"\"\"\n",
    "    Cambia la extensión de todos los archivos a .txt.\n",
    "    \n",
    "    Parámetros:\n",
    "        folder_path (str): Ruta del directorio donde se encuentran los archivos.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"La carpeta '{folder_path}' no existe. Verifica la ruta.\")\n",
    "        return\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        old_path = os.path.join(folder_path, filename)\n",
    "        # Verificar si es un archivo regular y no una carpeta\n",
    "        if os.path.isfile(old_path):\n",
    "            # Cambiar la extensión a .txt\n",
    "            new_filename = f\"{filename}.txt\" if '.' not in filename else f\"{os.path.splitext(filename)[0]}.txt\"\n",
    "            new_path = os.path.join(folder_path, new_filename)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Archivo renombrado: {old_path} -> {new_path}\")\n",
    "        else:\n",
    "            print(f\"Omitido (no es un archivo): {old_path}\")\n",
    "\n",
    "# Ruta de la carpeta principal descomprimida\n",
    "reuters_dir = r\"..\\data\\reuters\" \n",
    "\n",
    "# Cambiar extensiones en las carpetas training y test\n",
    "training_dir = os.path.join(reuters_dir, \"training\")\n",
    "test_dir = os.path.join(reuters_dir, \"test\")\n",
    "\n",
    "print(\"Procesando carpeta 'training'...\")\n",
    "change_extension_to_txt(training_dir)\n",
    "\n",
    "print(\"Procesando carpeta 'test'...\")\n",
    "change_extension_to_txt(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc9a5c2-7e00-40eb-af17-c13d9cce134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cats_file(cats_file_path):\n",
    "    \"\"\"\n",
    "    Lee el archivo cats.txt y crea un diccionario con categorías por nombre y origen.\n",
    "    \"\"\"\n",
    "    categories = {}\n",
    "    with open(cats_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            origin_and_name = parts[0] \n",
    "            origin, name = origin_and_name.split('/')\n",
    "            category_list = \" \".join(parts[1:])\n",
    "            categories[(origin, name)] = category_list\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740295c-2aee-474c-aca6-1c11a5aba7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_document_info(folder_path, origin, categories_dict):\n",
    "    \"\"\"\n",
    "    Extrae la información relevante de los documentos dentro de una carpeta.\n",
    "    \"\"\"\n",
    "    documents_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                lines = f.readlines()\n",
    "                title = lines[0].strip() if lines else ''\n",
    "                content = \"\".join(lines[1:]).strip()\n",
    "                category = categories_dict.get((origin, filename.split('.')[0]), '')\n",
    "                documents_data.append({\n",
    "                    'Nombre': filename.split('.')[0],\n",
    "                    'Titulo': title,\n",
    "                    'Contenido': content,\n",
    "                    'Origen': origin,\n",
    "                    'Categoria': category\n",
    "                })\n",
    "    return documents_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e0ddac-4e98-4406-997c-4a779ce6310a",
   "metadata": {},
   "source": [
    "## 2.2. Preprocesamiento\n",
    "### Objetivo: Limpiar y preparar los datos para su an´alisis.\n",
    "- Tareas:\n",
    "    - Extraer el contenido relevante de los documentos.\n",
    "    - Realizar limpieza de datos: eliminaci´on de caracteres no deseados, normalizaci´on de texto, etc.\n",
    "    - Tokenizaci´on: dividir el texto en palabras o tokens.\n",
    "    - Eliminar stop words y aplicar stemming o lematizaci´on.\n",
    "    - Documentar cada paso del preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14be197-702c-4211-8764-d7c70c6573c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Elimina caracteres no deseados y normaliza el texto.\"\"\"\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Eliminar puntuación\n",
    "    text = text.strip()  # Eliminar espacios iniciales y finales\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113aede-1681-4872-8ac7-622d2ba9701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(content):\n",
    "    \"\"\"Realiza la limpieza, tokenización, eliminación de stopwords y stemming del texto.\"\"\"\n",
    "    # Limpieza de texto\n",
    "    cleaned_text = clean_text(content)\n",
    "    \n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    \n",
    "    # Eliminación de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Aplicación de stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Reconstrucción del texto preprocesado\n",
    "    preprocessed_text = \" \".join(stemmed_tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f75f4a-9fa1-4551-bd80-058fdd1628ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de documentos\n",
    "def preprocess_documents(data):\n",
    "    \"\"\"\n",
    "    Aplica preprocesamiento al contenido de cada documento en los datos.\n",
    "    \"\"\"\n",
    "    for doc in data:\n",
    "        original_content = doc['Contenido']\n",
    "        preprocessed_content = preprocess_text(original_content)\n",
    "        doc['Contenido Preprocesado'] = preprocessed_content  # Agregar texto preprocesado\n",
    "        original_title = doc['Titulo']\n",
    "        preprocesse_title = preprocess_text(original_title)\n",
    "        doc['Titulo Preprocesado'] = preprocesse_title\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70cae1b-82ef-4a76-bca0-eb488d660fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas\n",
    "training_dir = os.path.join(reuters_dir, \"training\")\n",
    "test_dir = os.path.join(reuters_dir, \"test\")\n",
    "cats_file_path = os.path.join(reuters_dir, \"cats.txt\")\n",
    "\n",
    "# Leer el archivo cats.txt para obtener las categorías\n",
    "categories_dict = parse_cats_file(cats_file_path)\n",
    "\n",
    "# Procesar carpetas training y test\n",
    "training_data = extract_document_info(training_dir, \"training\", categories_dict)\n",
    "test_data = extract_document_info(test_dir, \"test\", categories_dict)\n",
    "\n",
    "# Preprocesar el contenido de los documentos\n",
    "all_data = training_data + test_data\n",
    "all_data = preprocess_documents(all_data)\n",
    "\n",
    "# Guardar en un archivo Excel\n",
    "df = pd.DataFrame(all_data) # Convierto a dataframe\n",
    "output_excel_path = os.path.join(reuters_dir, \"reuters_data_preprocessed.xlsx\")\n",
    "df.to_excel(output_excel_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542313c-b373-41fb-a9b8-34efcb3be9de",
   "metadata": {},
   "source": [
    "## 2.3. Representaci´on de Datos en Espacio Vectorial\n",
    "### Objetivo: Convertir los textos en una forma que los algoritmos puedan procesar.\n",
    "- Tareas:\n",
    "    - Utilizar t´ecnicas como Bag ofWords (BoW), TF-IDF, yWord2Vec para vectorizar el texto.\n",
    "    - Evaluar las diferentes t´ecnicas de vectorizaci´on.\n",
    "    - Documentar los m´etodos y resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34786beb-8bfb-4a04-b797-6af9b9023307",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_excel_path = os.path.join(reuters_dir, \"reuters_data_preprocessed.xlsx\")\n",
    "df = pd.read_excel(input_excel_path)\n",
    "\n",
    "# Seleccionar el contenido preprocesado\n",
    "texts = df['Contenido Preprocesado'].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19203e8-46b9-4407-96d8-8024ab0476b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words (BoW)\n",
    "def bag_of_words(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_matrix = vectorizer.fit_transform(texts)\n",
    "    bow_features = vectorizer.get_feature_names_out()\n",
    "    print(f\"BoW: Matriz de tamaño {bow_matrix.shape}\")\n",
    "    return bow_matrix, bow_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19400f28-9b20-48db-b282-b6a6bc89e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "def tf_idf(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    tfidf_features = vectorizer.get_feature_names_out()\n",
    "    print(f\"TF-IDF: Matriz de tamaño {tfidf_matrix.shape}\")\n",
    "    return tfidf_matrix, tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc4041-5d4b-40cd-ad6e-b9cf035e3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "def word2vec(texts, vector_size=100, window=5, min_count=1):\n",
    "    tokenized_texts = [text.split() for text in texts]\n",
    "    model = Word2Vec(sentences=tokenized_texts, vector_size=vector_size, window=window, min_count=min_count)\n",
    "    word_vectors = model.wv\n",
    "    print(f\"Word2Vec: {len(word_vectors)} palabras representadas con vectores de tamaño {vector_size}\")\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddcd44-5942-4231-8939-0dedb183d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar representaciones\n",
    "bow_matrix, bow_features = bag_of_words(texts)\n",
    "tfidf_matrix, tfidf_features = tf_idf(texts)\n",
    "word_vectors = word2vec(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4b455-f447-4dfc-a6ce-829dd808b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentar resultados\n",
    "results = {\n",
    "    \"Técnica\": [\"Bag of Words\", \"TF-IDF\", \"Word2Vec\"],\n",
    "    \"Dimensión de Matriz\": [bow_matrix.shape, tfidf_matrix.shape, len(word_vectors)],\n",
    "    \"Tamaño de Vocabulario\": [len(bow_features), len(tfidf_features), len(word_vectors)]\n",
    "}\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf4c22-dab8-43f9-8742-993917d67798",
   "metadata": {},
   "source": [
    "## 2.4. Indexaci´on\n",
    "### Objetivo: Crear un ´ındice que permita b´usquedas eficientes.\n",
    "- Tareas:\n",
    "    - Construir un ´ındice invertido que mapee t´erminos a documentos.\n",
    "    - Implementar y optimizar estructuras de datos para el ´ındice.\n",
    "    - Documentar el proceso de construcci´on del ´ındice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54fedfe-8862-4dc9-a8ad-c60ea3714011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(documents):\n",
    "    \"\"\"\n",
    "    Construye un índice invertido que mapea términos a documentos.\n",
    "    \n",
    "    Parámetros:\n",
    "        documents (list): Lista de diccionarios con los datos de los documentos.\n",
    "    \n",
    "    Retorna:\n",
    "        dict: Índice invertido donde las claves son términos y los valores son listas de documentos.\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(set)  # Diccionario donde cada término apunta a un conjunto de IDs de documentos\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_id = doc['Nombre']  # ID del documento\n",
    "        content = doc['Contenido Preprocesado']  # Contenido preprocesado\n",
    "        terms = set(content.split())  # Obtener términos únicos del documento\n",
    "        \n",
    "        for term in terms:\n",
    "            inverted_index[term].add(doc_id)  # Asocia el término con el documento\n",
    "    \n",
    "    return {term: list(doc_ids) for term, doc_ids in inverted_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c881e1-a273-41dd-8513-bcf55f359bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_inverted_index_to_excel(inverted_index, output_path):\n",
    "    \"\"\"\n",
    "    Guarda el índice invertido en un archivo Excel para fácil visualización.\n",
    "    \n",
    "    Parámetros:\n",
    "        inverted_index (dict): Índice invertido.\n",
    "        output_path (str): Ruta del archivo Excel donde se guardará.\n",
    "    \"\"\"\n",
    "    index_data = [{\"Término\": term, \"Documentos\": \", \".join(doc_ids)} for term, doc_ids in inverted_index.items()]\n",
    "    df = pd.DataFrame(index_data)\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"Índice invertido guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74be809b-2e90-4e2d-af10-8f4495a318cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar los datos preprocesados\n",
    "documents = df.to_dict(orient='records')\n",
    "\n",
    "# Construir índice invertido\n",
    "inverted_index = build_inverted_index(documents)\n",
    "\n",
    "# Guardar resultados en Excel\n",
    "output_excel_path = os.path.join(reuters_dir, \"inverted_index.xlsx\")\n",
    "save_inverted_index_to_excel(inverted_index, output_excel_path)\n",
    "\n",
    "# Documentación del proceso\n",
    "print(\"Índice invertido creado con éxito.\")\n",
    "print(f\"Términos indexados: {len(inverted_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277aa3cc-9468-402e-a290-315a475f4737",
   "metadata": {},
   "source": [
    "## 2.5. Diseño del Motor de B´usqueda\n",
    "### Objetivo: Implementar la funcionalidad de b´usqueda.\n",
    "- Tareas:\n",
    "    - Desarrollar la l´ogica para procesar consultas de usuarios.\n",
    "    - Utilizar algoritmos de similitud como similitud coseno o Jaccard.\n",
    "    - Desarrollar un algoritmo de ranking para ordenar los resultados.\n",
    "    - Documentar la arquitectura y los algoritmos utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1ee76-5fb1-4afc-a530-16bf5f7a3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de consulta\n",
    "def preprocess_query(query, stop_words):\n",
    "    \"\"\"\n",
    "    Limpia y preprocesa la consulta ingresada por el usuario.\n",
    "    \"\"\"\n",
    "    query = query.lower().translate(str.maketrans('', '', string.punctuation))  # Limpieza básica\n",
    "    tokens = query.split()  # Tokenización\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Eliminación de stop words\n",
    "    return tokens\n",
    "\n",
    "# Cargar datos del índice invertido y documentos\n",
    "input_excel_path = os.path.join(reuters_dir, \"reuters_data_preprocessed.xlsx\")\n",
    "df = pd.read_excel(input_excel_path)\n",
    "\n",
    "documents = df['Contenido Preprocesado'].tolist()\n",
    "document_ids = df['Nombre'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c928ae-d688-4349-a878-0600952ac918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorización con TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "def search_query_cosine(query, tfidf_vectorizer, tfidf_matrix, document_ids, top_k=10):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda utilizando similitud coseno.\n",
    "    \"\"\"\n",
    "    query_vector = tfidf_vectorizer.transform([query])  # Vectorizar consulta\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Ordenar documentos por similitud descendente\n",
    "    ranked_indices = np.argsort(-cosine_similarities)[:top_k]\n",
    "    results = [(document_ids[i], cosine_similarities[i]) for i in ranked_indices if cosine_similarities[i] > 0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982050d-864a-4997-8de4-01e47fda7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query_tokens, doc_tokens):\n",
    "    \"\"\"\n",
    "    Calcula la similitud de Jaccard entre la consulta y un documento.\n",
    "    \"\"\"\n",
    "    intersection = len(set(query_tokens).intersection(set(doc_tokens)))\n",
    "    union = len(set(query_tokens).union(set(doc_tokens)))\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd5768-a98d-4822-8233-07874f268914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query_jaccard(query, documents, document_ids, top_k=10):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda utilizando el coeficiente de Jaccard.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    query_tokens = query.split()\n",
    "    for doc_id, doc_content in zip(document_ids, documents):\n",
    "        doc_tokens = doc_content.split()\n",
    "        score = jaccard_similarity(query_tokens, doc_tokens)\n",
    "        if score > 0:\n",
    "            results.append((doc_id, score))\n",
    "    \n",
    "    # Ordenar resultados por puntuación descendente\n",
    "    results = sorted(results, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992a2ad-add6-4f22-baf6-1fa38853a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_results(results, method=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Ordena los resultados de búsqueda con base en el método especificado.\n",
    "    \"\"\"\n",
    "    return sorted(results, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e289aa-344f-46b2-a925-81973e90337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar con una consulta\n",
    "user_query = \"BAHIA COCOA REVIEW\"\n",
    "preprocessed_query = \" \".join(preprocess_query(user_query, set(stopwords.words('english'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a12004-caf3-425a-9775-88fd48c67afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Búsqueda con similitud coseno\n",
    "cosine_results = search_query_cosine(preprocessed_query, tfidf_vectorizer, tfidf_matrix, document_ids)\n",
    "cosine_results_ranked = rank_results(cosine_results)\n",
    "\n",
    "# Búsqueda con similitud Jaccard\n",
    "jaccard_results = search_query_jaccard(preprocessed_query, documents, document_ids)\n",
    "jaccard_results_ranked = rank_results(jaccard_results, method=\"jaccard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7b1a0-71ff-4f48-a3f4-90da979daf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resultados\n",
    "print(\"Resultados con Cosine Similarity:\", cosine_results_ranked, \"\\n\")\n",
    "print(\"Resultados con Jaccard Similarity:\", jaccard_results_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9964e2-fd7e-4b68-83dc-1e0f8523314f",
   "metadata": {},
   "source": [
    "## 2.6. Evaluaci´on del Sistema\n",
    "### Objetivo: Medir la efectividad del sistema.\n",
    "- Tareas:\n",
    "    - Definir un conjunto de m´etricas de evaluaci´on (precisi´on, recall, F1-score).\n",
    "    - Realizar pruebas utilizando el conjunto de prueba del corpus.\n",
    "    - Comparar el rendimiento de diferentes configuraciones del sistema.\n",
    "    - Documentar los resultados y an´alisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b13531-ec95-46db-8c1b-a9810da23136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(retrieved_docs, relevant_docs):\n",
    "    \"\"\"\n",
    "    Calcula precisión, recall y F1-Score.\n",
    "    \n",
    "    Parámetros:\n",
    "        retrieved_docs (list): Lista de documentos recuperados.\n",
    "        relevant_docs (list): Lista de documentos relevantes esperados.\n",
    "    \n",
    "    Retorna:\n",
    "        dict: Diccionario con precisión, recall y F1-Score.\n",
    "    \"\"\"\n",
    "    retrieved_set = set(retrieved_docs)\n",
    "    relevant_set = set(relevant_docs)\n",
    "    \n",
    "    true_positives = len(retrieved_set & relevant_set)\n",
    "    precision = true_positives / len(retrieved_set) if retrieved_set else 0\n",
    "    recall = true_positives / len(relevant_set) if relevant_set else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af71d73-ab0f-4101-b3ad-a9cea29c77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(queries, relevant_docs_dict, search_function, **kwargs):\n",
    "    \"\"\"\n",
    "    Evalúa el sistema de búsqueda en base a consultas y documentos relevantes.\n",
    "    \n",
    "    Parámetros:\n",
    "        queries (list): Lista de consultas.\n",
    "        relevant_docs_dict (dict): Diccionario con documentos relevantes para cada consulta.\n",
    "        search_function (function): Función de búsqueda a evaluar.\n",
    "        kwargs: Argumentos adicionales para la función de búsqueda.\n",
    "    \n",
    "    Retorna:\n",
    "        DataFrame: Resultados de precisión, recall y F1-Score por consulta.\n",
    "    \"\"\"\n",
    "    evaluation_results = []\n",
    "    \n",
    "    for query, relevant_docs in relevant_docs_dict.items():\n",
    "        results = search_function(query, **kwargs)\n",
    "        retrieved_docs = [doc_id for doc_id, _ in results]\n",
    "        metrics = calculate_metrics(retrieved_docs, relevant_docs)\n",
    "        metrics[\"Consulta\"] = query\n",
    "        evaluation_results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488db3ce-8aac-4ec6-a540-63db71f86006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación con similitud coseno\n",
    "cosine_eval_results = evaluate_system(\n",
    "    queries,\n",
    "    relevant_docs_dict,\n",
    "    search_query_cosine,\n",
    "    tfidf_vectorizer=tfidf_vectorizer,\n",
    "    tfidf_matrix=tfidf_matrix,\n",
    "    document_ids=document_ids\n",
    ")\n",
    "\n",
    "# Evaluación con similitud Jaccard\n",
    "jaccard_eval_results = evaluate_system(\n",
    "    queries,\n",
    "    relevant_docs_dict,\n",
    "    search_query_jaccard,\n",
    "    documents=documents,\n",
    "    document_ids=document_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1425f55-7ccd-4e24-b6f1-99aa9f44b3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8146f93-42f4-4080-ab3b-cf711cdebf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un mapeo entre identificadores abstractos (doc1, doc2, ...) y nombres reales\n",
    "doc_id_to_name = {f\"doc{index+1}\": str(doc_id) for index, doc_id in enumerate(df['Nombre'])}\n",
    "\n",
    "# Actualizar documentos relevantes esperados con nombres reales\n",
    "relevant_docs_dict_real = {\n",
    "    query: [doc_id_to_name[doc] for doc in relevant_docs if doc in doc_id_to_name]\n",
    "    for query, relevant_docs in relevant_docs_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5ab2b-9c4f-4636-bc1d-a4846867e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar la búsqueda y métricas\n",
    "for query, relevant_docs in relevant_docs_dict_real.items():\n",
    "    # Ejecutar la búsqueda\n",
    "    results = search_query_cosine(query, tfidf_vectorizer=tfidf_vectorizer, tfidf_matrix=tfidf_matrix, document_ids=document_ids)\n",
    "    retrieved_docs = [doc_id for doc_id, _ in results]\n",
    "    \n",
    "    # Calcular métricas\n",
    "    metrics = calculate_metrics(retrieved_docs, relevant_docs)\n",
    "    \n",
    "    # Imprimir detalles\n",
    "    print(f\"Consulta: {query}\")\n",
    "    print(f\"Documentos relevantes esperados: {relevant_docs}\")\n",
    "    print(f\"Documentos recuperados: {retrieved_docs}\")\n",
    "    print(f\"Métricas: Precisión={metrics['Precision']}, Recall={metrics['Recall']}, F1-Score={metrics['F1-Score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9063817-1352-4e0b-908d-90e2b18d0e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbfe1c4-b032-42b2-981d-3d60bc0918d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b0f4a-cd3a-4187-9bba-df606f11b311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacbd5e8-b8df-4dc3-a9b3-8af48bdde06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7f916-da03-44aa-be95-df932f841041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
