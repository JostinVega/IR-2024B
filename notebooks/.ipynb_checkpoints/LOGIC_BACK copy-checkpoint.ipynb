{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbed29f-1887-4c82-ad78-1c16b2788b88",
   "metadata": {},
   "source": [
    "# Proyecto Bimestral: Sistema de Recuperación de Información basado en Reuters-21578\n",
    "\n",
    "## 1. Introducci´on\n",
    "El objetivo de este proyecto es diseñar, construir, programar y desplegar un Sistema de Recuperación de Información (SRI) utilizando el corpus Reuters-21578. El proyecto se dividirá en varias fases,\n",
    "que se describen a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123359bb-8b07-4012-b5da-09aa94005cc2",
   "metadata": {},
   "source": [
    "## 2. Fases del Proyecto\n",
    "### 2.1. Adquisici´on de Datos\n",
    "- Objetivo: Obtener y preparar el corpus Reuters-21578.\n",
    "- Tareas:\n",
    "    - Descargar el corpus Reuters-21578.\n",
    "    - Descomprimir y organizar los archivos.\n",
    "    - Documentar el proceso de adquisici´on de datos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa698a8-bc9e-4e1f-9d7c-ec3ae1897fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos descomprimidos en: ..\\data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Descargar y descomprimir el corpus Reuters-21578\n",
    "def extract_reuters_data(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"Datos descomprimidos en:\", extract_to)\n",
    "\n",
    "# Ruta al archivo zip descargado y carpeta de destino\n",
    "zip_path = r\"..\\data\\reuters.zip\"\n",
    "extract_to = r\"..\\data\"\n",
    "extract_reuters_data(zip_path, extract_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d0242-65f1-417a-b77b-0b5564fe7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def change_extension_to_txt(folder_path):\n",
    "    \"\"\"\n",
    "    Cambia la extensión de todos los archivos en el directorio especificado a .txt.\n",
    "    \n",
    "    Parámetros:\n",
    "        folder_path (str): Ruta del directorio donde se encuentran los archivos.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"La carpeta '{folder_path}' no existe. Verifica la ruta.\")\n",
    "        return\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        old_path = os.path.join(folder_path, filename)\n",
    "        # Verificar si es un archivo regular y no una carpeta\n",
    "        if os.path.isfile(old_path):\n",
    "            # Cambiar la extensión a .txt\n",
    "            new_filename = f\"{filename}.txt\" if '.' not in filename else f\"{os.path.splitext(filename)[0]}.txt\"\n",
    "            new_path = os.path.join(folder_path, new_filename)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Archivo renombrado: {old_path} -> {new_path}\")\n",
    "        else:\n",
    "            print(f\"Omitido (no es un archivo): {old_path}\")\n",
    "\n",
    "# Ruta de la carpeta principal descomprimida\n",
    "reuters_dir = r\"..\\data\\reuters\"  # Cambiar según el directorio principal tras descomprimir\n",
    "\n",
    "# Cambiar extensiones en las carpetas training y test\n",
    "training_dir = os.path.join(reuters_dir, \"training\")\n",
    "test_dir = os.path.join(reuters_dir, \"test\")\n",
    "\n",
    "print(\"Procesando carpeta 'training'...\")\n",
    "change_extension_to_txt(training_dir)\n",
    "\n",
    "print(\"Procesando carpeta 'test'...\")\n",
    "change_extension_to_txt(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fee9f-81e7-45b0-96e3-5f76e435b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files_with_prefix(folder_path, prefix):\n",
    "    \"\"\"\n",
    "    Renombra los archivos en una carpeta agregando un prefijo al nombre del archivo.\n",
    "    \n",
    "    Parámetros:\n",
    "        folder_path (str): Ruta de la carpeta donde se encuentran los archivos.\n",
    "        prefix (str): Prefijo lógico que se imprime, pero no forma parte del nombre del archivo.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"La carpeta '{folder_path}' no existe. Verifica la ruta.\")\n",
    "        return\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        old_path = os.path.join(folder_path, filename)\n",
    "        # Verificar si es un archivo regular y no una carpeta\n",
    "        if os.path.isfile(old_path):\n",
    "            # Crear nuevo nombre sin incluir el prefijo en la ruta\n",
    "            new_filename = filename  # El prefijo es solo lógico, no afecta al nombre físico\n",
    "            new_path = os.path.join(folder_path, new_filename)\n",
    "            \n",
    "            # Renombrar archivo (en este caso, puede ser el mismo nombre)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Archivo renombrado: {prefix}{filename} -> {prefix}{new_filename}\")\n",
    "        else:\n",
    "            print(f\"Omitido (no es un archivo): {old_path}\")\n",
    "\n",
    "# Ruta de la carpeta principal descomprimida\n",
    "reuters_dir = r\"..\\data\\reuters\"  # Cambiar según el directorio principal tras descomprimir\n",
    "\n",
    "# Renombrar archivos en las carpetas training y test\n",
    "training_dir = os.path.join(reuters_dir, \"training\")\n",
    "test_dir = os.path.join(reuters_dir, \"test\")\n",
    "\n",
    "print(\"Renombrando archivos en 'training'...\")\n",
    "rename_files_with_prefix(training_dir, \"training/\")\n",
    "\n",
    "print(\"Renombrando archivos en 'test'...\")\n",
    "rename_files_with_prefix(test_dir, \"test/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb961f76-ca72-4c67-8b8e-06b772da109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def parse_cats_file(cats_file_path):\n",
    "    \"\"\"\n",
    "    Lee el archivo cats.txt y crea un diccionario con categorías por nombre y origen.\n",
    "    \"\"\"\n",
    "    categories = {}\n",
    "    with open(cats_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            origin_and_name = parts[0]  # ejemplo: test/14826\n",
    "            origin, name = origin_and_name.split('/')\n",
    "            category_list = \" \".join(parts[1:])  # categorías separadas por espacios\n",
    "            categories[(origin, name)] = category_list\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b52b55a-7bb7-4937-b78c-8320606b3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_document_info(folder_path, origin, categories_dict):\n",
    "    \"\"\"\n",
    "    Extrae la información relevante de los documentos dentro de una carpeta.\n",
    "    \"\"\"\n",
    "    documents_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                lines = f.readlines()\n",
    "                title = lines[0].strip() if lines else ''\n",
    "                content = \"\".join(lines[1:]).strip()\n",
    "                category = categories_dict.get((origin, filename.split('.')[0]), '')\n",
    "                documents_data.append({\n",
    "                    'Nombre': filename.split('.')[0],\n",
    "                    'Titulo': title,\n",
    "                    'Contenido': content,\n",
    "                    'Origen': origin,\n",
    "                    'Categoria': category\n",
    "                })\n",
    "    return documents_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7e86ab3-a08a-431d-bf87-6627ef31d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta\n",
    "cats_file_path = os.path.join(reuters_dir, \"cats.txt\")\n",
    "\n",
    "# Leer el archivo cats.txt para obtener las categorías\n",
    "categories_dict = parse_cats_file(cats_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeb30cfd-6b7d-4e38-812c-d09fda4d362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar carpetas training y test\n",
    "training_dir = os.path.join(reuters_dir, \"training\")\n",
    "test_dir = os.path.join(reuters_dir, \"test\")\n",
    "\n",
    "training_data = extract_document_info(training_dir, \"training\", categories_dict)\n",
    "test_data = extract_document_info(test_dir, \"test\", categories_dict)\n",
    "\n",
    "# Combinar los datos en un DataFrame\n",
    "all_data = training_data + test_data\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Guardar en un archivo Excel\n",
    "output_excel_path = os.path.join(reuters_dir, \"reuters_data.xlsx\")\n",
    "df.to_excel(output_excel_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e208fea-ccfc-42bb-b363-8bc62cd09598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48aca2f1-c79d-4711-85f5-38a37949f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de preprocesamiento\n",
    "def clean_text(text):\n",
    "    \"\"\"Elimina caracteres no deseados y normaliza el texto.\"\"\"\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Eliminar puntuación\n",
    "    text = text.strip()  # Eliminar espacios iniciales y finales\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db4b4104-c42f-4c22-85e1-e39e577c9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(content):\n",
    "    \"\"\"Realiza la limpieza, tokenización, eliminación de stopwords y stemming del texto.\"\"\"\n",
    "    # Limpieza de texto\n",
    "    cleaned_text = clean_text(content)\n",
    "    \n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    \n",
    "    # Eliminación de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Aplicación de stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Reconstrucción del texto preprocesado\n",
    "    preprocessed_text = \" \".join(stemmed_tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_without_stemmer(content):\n",
    "    \"\"\"Realiza la limpieza, tokenización, eliminación de stopwords y stemming del texto.\"\"\"\n",
    "    # Limpieza de texto\n",
    "    cleaned_text = clean_text(content)\n",
    "    \n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    \n",
    "    # Eliminación de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Reconstrucción del texto preprocesado\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2430b-c1e1-4a28-82fd-dfd7d9ccdfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de documentos\n",
    "def preprocess_documents(data):\n",
    "    \"\"\"\n",
    "    Aplica preprocesamiento al contenido de cada documento en los datos.\n",
    "    \"\"\"\n",
    "    for doc in data:\n",
    "        original_content = doc['Contenido']\n",
    "        preprocessed_content = preprocess_text(original_content)\n",
    "        doc['Contenido Preprocesado'] = preprocessed_content  # Agregar texto preprocesado\n",
    "        preprocessed_content_wo_stemmer = preprocess_text_without_stemmer(original_contente)\n",
    "        doc['Contenido_Preprocesado_Sin_Stemmer'] = preprocess_text_without_stemmer\n",
    "        original_title = doc['Titulo']\n",
    "        preprocesse_title = preprocess_text(original_title)\n",
    "        doc['Titulo Preprocesado'] = preprocesse_title\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86e983fe-03b5-414a-8eeb-427fa2822939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas\n",
    "reuters_dir = r\"..\\data\\reuters\"\n",
    "cats_file_path = os.path.join(reuters_dir, \"cats.txt\")\n",
    "\n",
    "# Leer el archivo cats.txt para obtener las categorías\n",
    "categories_dict = parse_cats_file(cats_file_path)\n",
    "\n",
    "# Procesar carpetas training y test\n",
    "training_dir = os.path.join(reuters_dir, \"training\")\n",
    "test_dir = os.path.join(reuters_dir, \"test\")\n",
    "\n",
    "training_data = extract_document_info(training_dir, \"training\", categories_dict)\n",
    "test_data = extract_document_info(test_dir, \"test\", categories_dict)\n",
    "\n",
    "# Combinar los datos en un solo conjunto\n",
    "all_data = training_data + test_data\n",
    "\n",
    "# Preprocesar el contenido de los documentos\n",
    "all_data = preprocess_documents(all_data)\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Guardar en un archivo Excel\n",
    "output_excel_path = os.path.join(reuters_dir, \"reuters_data_preprocessed.xlsx\")\n",
    "df.to_excel(output_excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d49850-08d0-4c11-b30a-32af794cd2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1caa8fb2-1a82-4a6b-9d40-fa1237b2761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos preprocesados\n",
    "reuters_dir = r\"..\\data\\reuters\"\n",
    "input_excel_path = os.path.join(reuters_dir, \"reuters_data_preprocessed.xlsx\")\n",
    "df = pd.read_excel(input_excel_path)\n",
    "\n",
    "# Seleccionar el contenido preprocesado\n",
    "texts = df['Contenido Preprocesado'].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "929a4895-b3be-4a3a-9bd7-77fdfe949c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words (BoW)\n",
    "def bag_of_words(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_matrix = vectorizer.fit_transform(texts)\n",
    "    bow_features = vectorizer.get_feature_names_out()\n",
    "    print(f\"BoW: Matriz de tamaño {bow_matrix.shape}\")\n",
    "    return bow_matrix, bow_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ed43cca-a9e6-40ab-8ae2-9c3813a2a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "def tf_idf(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    tfidf_features = vectorizer.get_feature_names_out()\n",
    "    print(f\"TF-IDF: Matriz de tamaño {tfidf_matrix.shape}\")\n",
    "    return tfidf_matrix, tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de8ddf49-9187-422b-bb41-6690477aa967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "def word2vec(texts, vector_size=100, window=5, min_count=1):\n",
    "    tokenized_texts = [text.split() for text in texts]\n",
    "    model = Word2Vec(sentences=tokenized_texts, vector_size=vector_size, window=window, min_count=min_count)\n",
    "    word_vectors = model.wv\n",
    "    print(f\"Word2Vec: {len(word_vectors)} palabras representadas con vectores de tamaño {vector_size}\")\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "797e676c-bea4-4919-b801-579d5511d034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW: Matriz de tamaño (10789, 38328)\n",
      "TF-IDF: Matriz de tamaño (10789, 38328)\n",
      "Word2Vec: 38355 palabras representadas con vectores de tamaño 100\n"
     ]
    }
   ],
   "source": [
    "# Generar representaciones\n",
    "bow_matrix, bow_features = bag_of_words(texts)\n",
    "tfidf_matrix, tfidf_features = tf_idf(texts)\n",
    "word_vectors = word2vec(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2e08398-1595-460e-8b1f-f891a2b30ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentar resultados\n",
    "results = {\n",
    "    \"Técnica\": [\"Bag of Words\", \"TF-IDF\", \"Word2Vec\"],\n",
    "    \"Dimensión de Matriz\": [bow_matrix.shape, tfidf_matrix.shape, len(word_vectors)],\n",
    "    \"Tamaño de Vocabulario\": [len(bow_features), len(tfidf_features), len(word_vectors)]\n",
    "}\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e03e2bb1-738a-4638-9713-ec2850c945af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Técnica</th>\n",
       "      <th>Dimensión de Matriz</th>\n",
       "      <th>Tamaño de Vocabulario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>(10789, 38328)</td>\n",
       "      <td>38328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>(10789, 38328)</td>\n",
       "      <td>38328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>38355</td>\n",
       "      <td>38355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Técnica Dimensión de Matriz  Tamaño de Vocabulario\n",
       "0  Bag of Words      (10789, 38328)                  38328\n",
       "1        TF-IDF      (10789, 38328)                  38328\n",
       "2      Word2Vec               38355                  38355"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa402ff9-6932-4962-ad18-c8e5d52d3e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados en Excel\n",
    "results_excel_path = os.path.join(reuters_dir, \"vectorization_results.xlsx\")\n",
    "results_df.to_excel(results_excel_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be81aabb-63f3-4b20-a566-2b31e2f900cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    \"\"\"\n",
    "    Construye un índice invertido que mapea términos a documentos.\n",
    "    \n",
    "    Parámetros:\n",
    "        documents (list): Lista de diccionarios con los datos de los documentos.\n",
    "    \n",
    "    Retorna:\n",
    "        dict: Índice invertido donde las claves son términos y los valores son listas de documentos.\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(set)  # Diccionario donde cada término apunta a un conjunto de IDs de documentos\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_id = doc['Nombre']  # ID del documento\n",
    "        content = doc['Contenido Preprocesado']  # Contenido preprocesado\n",
    "        terms = set(content.split())  # Obtener términos únicos del documento\n",
    "        \n",
    "        for term in terms:\n",
    "            inverted_index[term].add(doc_id)  # Asocia el término con el documento\n",
    "    \n",
    "    return {term: list(doc_ids) for term, doc_ids in inverted_index.items()}\n",
    "\n",
    "def save_inverted_index_to_excel(inverted_index, output_path):\n",
    "    \"\"\"\n",
    "    Guarda el índice invertido en un archivo Excel para fácil visualización.\n",
    "    \n",
    "    Parámetros:\n",
    "        inverted_index (dict): Índice invertido.\n",
    "        output_path (str): Ruta del archivo Excel donde se guardará.\n",
    "    \"\"\"\n",
    "    index_data = [{\"Término\": term, \"Documentos\": \", \".join(doc_ids)} for term, doc_ids in inverted_index.items()]\n",
    "    df = pd.DataFrame(index_data)\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"Índice invertido guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74254402-07f4-4cf5-80a0-9b8c1c272095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos preprocesados\n",
    "reuters_dir = r\"..\\data\\reuters\"\n",
    "input_excel_path = os.path.join(reuters_dir, \"reuters_data_preprocessed.xlsx\")\n",
    "df = pd.read_excel(input_excel_path)\n",
    "\n",
    "# Seleccionar los datos preprocesados\n",
    "documents = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91c729bf-f870-4b08-aa83-a3b0f5f7049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice invertido guardado en: ..\\data\\reuters\\inverted_index.xlsx\n",
      "Índice invertido creado con éxito.\n",
      "Términos indexados: 38355\n"
     ]
    }
   ],
   "source": [
    "# Construir índice invertido\n",
    "inverted_index = build_inverted_index(documents)\n",
    "\n",
    "# Guardar resultados en Excel\n",
    "output_excel_path = os.path.join(reuters_dir, \"inverted_index.xlsx\")\n",
    "save_inverted_index_to_excel(inverted_index, output_excel_path)\n",
    "\n",
    "# Documentación del proceso\n",
    "print(\"Índice invertido creado con éxito.\")\n",
    "print(f\"Términos indexados: {len(inverted_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d4bdafc-07b3-4495-a4a5-5ab3e3e65b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocesamiento de consulta\n",
    "def preprocess_query(query, stop_words):\n",
    "    \"\"\"\n",
    "    Limpia y preprocesa la consulta ingresada por el usuario.\n",
    "    \"\"\"\n",
    "    query = query.lower().translate(str.maketrans('', '', string.punctuation))  # Limpieza básica\n",
    "    tokens = query.split()  # Tokenización\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Eliminación de stop words\n",
    "    return tokens\n",
    "\n",
    "# Cargar datos del índice invertido y documentos\n",
    "input_excel_path = os.path.join(reuters_dir, \"reuters_data_preprocessed.xlsx\")\n",
    "df = pd.read_excel(input_excel_path)\n",
    "\n",
    "documents = df['Contenido Preprocesado'].tolist()\n",
    "document_ids = df['Nombre'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "366a147c-23ef-4fc1-aea9-98eec1e05f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorización con TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "def search_query_cosine(query, tfidf_vectorizer, tfidf_matrix, document_ids, top_k=10):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda utilizando similitud coseno.\n",
    "    \"\"\"\n",
    "    query_vector = tfidf_vectorizer.transform([query])  # Vectorizar consulta\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Ordenar documentos por similitud descendente\n",
    "    ranked_indices = np.argsort(-cosine_similarities)[:top_k]\n",
    "    results = [(document_ids[i], cosine_similarities[i]) for i in ranked_indices if cosine_similarities[i] > 0]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71cd8bbd-b4e3-4c28-a274-56c0e450da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query_tokens, doc_tokens):\n",
    "    \"\"\"\n",
    "    Calcula la similitud de Jaccard entre la consulta y un documento.\n",
    "    \"\"\"\n",
    "    intersection = len(set(query_tokens).intersection(set(doc_tokens)))\n",
    "    union = len(set(query_tokens).union(set(doc_tokens)))\n",
    "    return intersection / union\n",
    "\n",
    "def search_query_jaccard(query, documents, document_ids, top_k=10):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda utilizando el coeficiente de Jaccard.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    query_tokens = query.split()\n",
    "    for doc_id, doc_content in zip(document_ids, documents):\n",
    "        doc_tokens = doc_content.split()\n",
    "        score = jaccard_similarity(query_tokens, doc_tokens)\n",
    "        if score > 0:\n",
    "            results.append((doc_id, score))\n",
    "    \n",
    "    # Ordenar resultados por puntuación descendente\n",
    "    results = sorted(results, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0199b7b-76d3-4d59-88b6-f2aab719b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_results(results, method=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Ordena los resultados de búsqueda con base en el método especificado.\n",
    "    \"\"\"\n",
    "    return sorted(results, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "07da3a91-7de8-4e45-8b6e-d61c427c8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar con una consulta\n",
    "user_query = \"BAHIA COCOA REVIEW\"\n",
    "preprocessed_query = \" \".join(preprocess_query(user_query, set(stopwords.words('english'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d534916f-d38c-47b8-b107-01f4b77a2b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bahia cocoa review'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9c9e373-252c-4d8e-b744-813359c30e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Búsqueda con similitud coseno\n",
    "cosine_results = search_query_cosine(preprocessed_query, tfidf_vectorizer, tfidf_matrix, document_ids)\n",
    "cosine_results_ranked = rank_results(cosine_results)\n",
    "\n",
    "# Búsqueda con similitud Jaccard\n",
    "jaccard_results = search_query_jaccard(preprocessed_query, documents, document_ids)\n",
    "jaccard_results_ranked = rank_results(jaccard_results, method=\"jaccard\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b574d52d-1b2e-4edf-ab32-7b323f17297c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con Cosine Similarity: [('10505', 0.3489190110955247), ('20005', 0.3299085008910773), ('5258', 0.3081515746454346), ('17568', 0.295433459938331), ('10506', 0.29295174708100014), ('9450', 0.29263056733129134), ('1', 0.28592552258074655), ('15095', 0.2815685169312497), ('9953', 0.2800541328507257), ('10760', 0.2726918249116274)] \n",
      "\n",
      "Resultados con Jaccard Similarity: [('10471', 0.1), ('10491', 0.1), ('21061', 0.1), ('6068', 0.09090909090909091), ('3190', 0.08333333333333333), ('8326', 0.07142857142857142), ('17733', 0.07142857142857142), ('18221', 0.07142857142857142), ('19358', 0.07142857142857142), ('6873', 0.045454545454545456)]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar resultados\n",
    "print(\"Resultados con Cosine Similarity:\", cosine_results_ranked, \"\\n\")\n",
    "print(\"Resultados con Jaccard Similarity:\", jaccard_results_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d694a-e122-4460-99a0-86145f3f1aa7",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07d5caa9-80d0-4f1c-bca6-91afa9c44e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(retrieved_docs, relevant_docs):\n",
    "    \"\"\"\n",
    "    Calcula precisión, recall y F1-Score.\n",
    "    \n",
    "    Parámetros:\n",
    "        retrieved_docs (list): Lista de documentos recuperados.\n",
    "        relevant_docs (list): Lista de documentos relevantes esperados.\n",
    "    \n",
    "    Retorna:\n",
    "        dict: Diccionario con precisión, recall y F1-Score.\n",
    "    \"\"\"\n",
    "    retrieved_set = set(retrieved_docs)\n",
    "    relevant_set = set(relevant_docs)\n",
    "    \n",
    "    true_positives = len(retrieved_set & relevant_set)\n",
    "    precision = true_positives / len(retrieved_set) if retrieved_set else 0\n",
    "    recall = true_positives / len(relevant_set) if relevant_set else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e0d26da-7f2c-42ce-ae3c-dd0387774b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(queries, relevant_docs_dict, search_function, **kwargs):\n",
    "    \"\"\"\n",
    "    Evalúa el sistema de búsqueda en base a consultas y documentos relevantes.\n",
    "    \n",
    "    Parámetros:\n",
    "        queries (list): Lista de consultas.\n",
    "        relevant_docs_dict (dict): Diccionario con documentos relevantes para cada consulta.\n",
    "        search_function (function): Función de búsqueda a evaluar.\n",
    "        kwargs: Argumentos adicionales para la función de búsqueda.\n",
    "    \n",
    "    Retorna:\n",
    "        DataFrame: Resultados de precisión, recall y F1-Score por consulta.\n",
    "    \"\"\"\n",
    "    evaluation_results = []\n",
    "    \n",
    "    for query, relevant_docs in relevant_docs_dict.items():\n",
    "        results = search_function(query, **kwargs)\n",
    "        retrieved_docs = [doc_id for doc_id, _ in results]\n",
    "        metrics = calculate_metrics(retrieved_docs, relevant_docs)\n",
    "        metrics[\"Consulta\"] = query\n",
    "        evaluation_results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Simulación de consultas y documentos relevantes\n",
    "queries = [\"cocoa trade\", \"market analysis\", \"price increase\"]\n",
    "relevant_docs_dict = {\n",
    "    \"cocoa trade\": [\"doc1\", \"doc2\"],\n",
    "    \"market analysis\": [\"doc3\"],\n",
    "    \"price increase\": [\"doc2\"]\n",
    "}\n",
    "\n",
    "# Evaluación con similitud coseno\n",
    "cosine_eval_results = evaluate_system(\n",
    "    queries,\n",
    "    relevant_docs_dict,\n",
    "    search_query_cosine,\n",
    "    tfidf_vectorizer=tfidf_vectorizer,\n",
    "    tfidf_matrix=tfidf_matrix,\n",
    "    document_ids=document_ids\n",
    ")\n",
    "\n",
    "# Evaluación con similitud Jaccard\n",
    "jaccard_eval_results = evaluate_system(\n",
    "    queries,\n",
    "    relevant_docs_dict,\n",
    "    search_query_jaccard,\n",
    "    documents=documents,\n",
    "    document_ids=document_ids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "afe41895-13e4-4347-bd75-9e92669a39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar métricas entre configuraciones\n",
    "comparison_df = pd.concat([\n",
    "    cosine_eval_results.assign(Método=\"Cosine Similarity\"),\n",
    "    jaccard_eval_results.assign(Método=\"Jaccard Similarity\")\n",
    "])\n",
    "\n",
    "# Guardar resultados en un archivo Excel\n",
    "output_comparison_path = os.path.join(reuters_dir, \"evaluation_results.xlsx\")\n",
    "comparison_df.to_excel(output_comparison_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c2674f7-bcb4-40d4-9b6c-49561e4d2bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta: cocoa trade\n",
      "Documentos relevantes esperados: ['1', '10']\n",
      "Documentos recuperados: ['10505', '20005', '5258', '10506', '10586', '15653', '10760', '15095', '9953', '9450']\n",
      "Consulta: market analysis\n",
      "Documentos relevantes esperados: ['100']\n",
      "Documentos recuperados: ['9415', '4392', '19989', '8168', '9261', '5896', '247', '3084', '8098', '16658']\n",
      "Consulta: price increase\n",
      "Documentos relevantes esperados: ['10']\n",
      "Documentos recuperados: ['19082', '5812', '18367', '4405', '4824', '6371', '18621', '7304', '10385', '5769']\n"
     ]
    }
   ],
   "source": [
    "# Crear un mapeo entre identificadores abstractos (doc1, doc2, ...) y nombres reales\n",
    "doc_id_to_name = {f\"doc{index+1}\": str(doc_id) for index, doc_id in enumerate(df['Nombre'])}\n",
    "\n",
    "# Revisar la búsqueda y resultados con nombres reales de los documentos\n",
    "for query, relevant_docs in relevant_docs_dict.items():\n",
    "    # Convertir los \"documentos relevantes esperados\" a nombres reales\n",
    "    relevant_doc_names = [doc_id_to_name[doc] for doc in relevant_docs if doc in doc_id_to_name]\n",
    "    \n",
    "    # Ejecutar la búsqueda\n",
    "    results = search_query_cosine(query, tfidf_vectorizer=tfidf_vectorizer, tfidf_matrix=tfidf_matrix, document_ids=document_ids)\n",
    "    retrieved_docs = [doc_id for doc_id, _ in results]\n",
    "    \n",
    "    print(f\"Consulta: {query}\")\n",
    "    print(f\"Documentos relevantes esperados: {relevant_doc_names}\")\n",
    "    print(f\"Documentos recuperados: {retrieved_docs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "763903db-a906-4694-861f-d2bfb261749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un mapeo entre identificadores abstractos (doc1, doc2, ...) y nombres reales\n",
    "doc_id_to_name = {f\"doc{index+1}\": str(doc_id) for index, doc_id in enumerate(df['Nombre'])}\n",
    "\n",
    "# Actualizar documentos relevantes esperados con nombres reales\n",
    "relevant_docs_dict_real = {\n",
    "    query: [doc_id_to_name[doc] for doc in relevant_docs if doc in doc_id_to_name]\n",
    "    for query, relevant_docs in relevant_docs_dict.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "21006cf0-2016-44eb-aeb7-639ec0ba16bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta: cocoa trade\n",
      "Documentos relevantes esperados: ['1', '10']\n",
      "Documentos recuperados: ['10505', '20005', '5258', '10506', '10586', '15653', '10760', '15095', '9953', '9450']\n",
      "Métricas: Precisión=0.0, Recall=0.0, F1-Score=0\n",
      "Consulta: market analysis\n",
      "Documentos relevantes esperados: ['100']\n",
      "Documentos recuperados: ['9415', '4392', '19989', '8168', '9261', '5896', '247', '3084', '8098', '16658']\n",
      "Métricas: Precisión=0.0, Recall=0.0, F1-Score=0\n",
      "Consulta: price increase\n",
      "Documentos relevantes esperados: ['10']\n",
      "Documentos recuperados: ['19082', '5812', '18367', '4405', '4824', '6371', '18621', '7304', '10385', '5769']\n",
      "Métricas: Precisión=0.0, Recall=0.0, F1-Score=0\n"
     ]
    }
   ],
   "source": [
    "# Revisar la búsqueda y métricas\n",
    "for query, relevant_docs in relevant_docs_dict_real.items():\n",
    "    # Ejecutar la búsqueda\n",
    "    results = search_query_cosine(query, tfidf_vectorizer=tfidf_vectorizer, tfidf_matrix=tfidf_matrix, document_ids=document_ids)\n",
    "    retrieved_docs = [doc_id for doc_id, _ in results]\n",
    "    \n",
    "    # Calcular métricas\n",
    "    metrics = calculate_metrics(retrieved_docs, relevant_docs)\n",
    "    \n",
    "    # Imprimir detalles\n",
    "    print(f\"Consulta: {query}\")\n",
    "    print(f\"Documentos relevantes esperados: {relevant_docs}\")\n",
    "    print(f\"Documentos recuperados: {retrieved_docs}\")\n",
    "    print(f\"Métricas: Precisión={metrics['Precision']}, Recall={metrics['Recall']}, F1-Score={metrics['F1-Score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "727195f7-ee38-4eea-bdfe-3e2db6dfcdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from flask) (2.0.1)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from flask) (2.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from click>=8.0->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->flask) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03976aeb-e769-4763-859d-2895dcc6ee78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb234ef4-fe55-4c0c-be2b-69f3923a16fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c79c9b-a5aa-4006-8676-229b6c2d19a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9552d1-7652-446e-a1e1-c7a73b5a30fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201c0da-0261-4558-b2ae-2bf83bd50378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44913e2d-5f6a-4c37-8974-fce168887d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a96c78-0ede-4013-99fd-e9aee0edec3e",
   "metadata": {},
   "source": [
    "# Carga del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6c662c-6a6f-4aef-aa9b-302973f97c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio donde se encuentran los archivos\n",
    "directorytraining = extract_to + r\"\\reuters\\training\"\n",
    "directorytest = extract_to + r\"\\reuters\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb16ad6-24df-4d8d-9a3c-e20451171080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar las carpetas training y test dentro de la carpeta principal\n",
    "    for subfolder, prefix in [(\"training\", \"training/\"), (\"test\", \"test/\")]:\n",
    "        folder_path = os.path.join(reuters_dir, subfolder)\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"El directorio '{folder_path}' no existe. Saltando.\")\n",
    "            continue\n",
    "        \n",
    "        # Recorrer y renombrar archivos\n",
    "        for filename in os.listdir(folder_path):\n",
    "            old_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(old_path):\n",
    "                # Crear nuevo nombre con prefijo y extensión .txt\n",
    "                new_filename = f\"{prefix}{os.path.splitext(filename)[0]}.txt\"\n",
    "                new_path = os.path.join(folder_path, new_filename)\n",
    "                \n",
    "                try:\n",
    "                    # Renombrar archivo\n",
    "                    os.rename(old_path, new_path)\n",
    "                    print(f\"Archivo renombrado: {old_path} -> {new_path}\")\n",
    "                except FileNotFoundError as e:\n",
    "                    print(f\"Error al renombrar archivo: {old_path} -> {new_path}. Detalles: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inesperado con el archivo {old_path}. Detalles: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ae8dd8-c187-4b59-baf3-37e7808d85a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos cargados: 7769\n"
     ]
    }
   ],
   "source": [
    "# Leer cada archivo de texto en el directorio\n",
    "documents = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\") or filename.isnumeric():  # Ajusta la condición según el formato del nombre\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            documents.append(content)\n",
    "\n",
    "print(\"Total de documentos cargados:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15b7b0-e6d5-4a2d-bcb3-c468e3dc9bf7",
   "metadata": {},
   "source": [
    "## 2.2. Preprocesamiento\n",
    "- Objetivo: Limpiar y preparar los datos para su an´alisis.\n",
    "- Tareas:\n",
    "    - Extraer el contenido relevante de los documentos.\n",
    "    - Realizar limpieza de datos: eliminaci´on de caracteres no deseados, normalizaci´on de texto, etc.\n",
    "    - Tokenizaci´on: dividir el texto en palabras o tokens.\n",
    "    - Eliminar stop words y aplicar stemming o lematizaci´on.\n",
    "    - Documentar cada paso del preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f711fe7-b056-4f4c-b1c6-011b1a9e4fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampl document preprocess inform retriev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Cargar stopwords y el stemmer\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Función de preprocesamiento\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Eliminar caracteres no deseados\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    tokens = text.split()  # Tokenizar el texto\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Quitar stopwords\n",
    "    tokens = [stemmer.stem(word) for word in tokens]  # Aplicar stemming\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Ejemplo de uso\n",
    "sample_text = \"This is a sample document for preprocessing in Information Retrieval.\"\n",
    "processed_text = preprocess_text(sample_text)\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f9d0788-8670-474e-ab8b-b8c374c3aa39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sampl document preprocess inform retriev'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da94f2-a595-46b4-8ea3-28dcf2b9b670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74926260-8412-49e7-bdfa-77b46b824a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9edfb92-494a-4efb-8a55-48e70f00be83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12873a5-40f0-4455-8e5f-fc711b0dc69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136c7d9-3e59-4ecc-888d-56c20fb32fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb958ee-783c-42e7-ba83-a79ec443d795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891b43d-8b3a-443c-8649-89f936a87ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daaa878f-3c59-42a0-ac15-75098301a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ReutersImplementation:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Inicializa el sistema con la ruta a los datos\n",
    "        Args:\n",
    "            data_path (str): Ruta al directorio con los archivos Reuters\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.documents = []\n",
    "        self.processed_docs = []\n",
    "        self.index = {}\n",
    "        self.vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        \n",
    "        # Inicializar NLTK\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            print(\"Descargando recursos NLTK necesarios...\")\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "    def load_single_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Carga y procesa un único archivo Reuters\n",
    "        Args:\n",
    "            file_path (str): Ruta al archivo\n",
    "        Returns:\n",
    "            list: Lista de documentos extraídos del archivo\n",
    "        \"\"\"\n",
    "        docs = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "                # Limpiar el contenido para manejar casos especiales de Reuters\n",
    "                content = content.replace('&', '&amp;')\n",
    "                content = re.sub(r'<!DOCTYPE[^>]*>', '', content)\n",
    "                content = re.sub(r'<!DOCTYPE[^>]*>', '', content)\n",
    "                \n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                reuters_docs = soup.find_all('reuters')\n",
    "                \n",
    "                for doc in reuters_docs:\n",
    "                    # Extraer campos relevantes\n",
    "                    doc_id = doc.get('newid', '')\n",
    "                    \n",
    "                    # Extraer título\n",
    "                    title_tag = doc.find('title')\n",
    "                    title = title_tag.get_text().strip() if title_tag else ''\n",
    "                    \n",
    "                    # Extraer cuerpo\n",
    "                    body_tag = doc.find('body')\n",
    "                    body = body_tag.get_text().strip() if body_tag else ''\n",
    "                    \n",
    "                    # Extraer tópicos\n",
    "                    topics_tag = doc.find('topics')\n",
    "                    topics = [t.get_text().strip() for t in doc.find_all('d')] if topics_tag else []\n",
    "                    \n",
    "                    if body:  # Solo incluir documentos que tengan cuerpo\n",
    "                        docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'title': title,\n",
    "                            'body': body,\n",
    "                            'topics': topics,\n",
    "                            'file_name': os.path.basename(file_path)\n",
    "                        })\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando archivo {file_path}: {str(e)}\")\n",
    "            \n",
    "        return docs\n",
    "\n",
    "    def load_corpus(self):\n",
    "        \"\"\"\n",
    "        Carga todo el corpus de Reuters\n",
    "        \"\"\"\n",
    "        print(\"Cargando corpus Reuters...\")\n",
    "        files = [f for f in os.listdir(self.data_path) if f.endswith('.sgm')]\n",
    "        \n",
    "        for file_name in tqdm(files, desc=\"Procesando archivos\"):\n",
    "            file_path = os.path.join(self.data_path, file_name)\n",
    "            docs = self.load_single_file(file_path)\n",
    "            self.documents.extend(docs)\n",
    "            \n",
    "        print(f\"Total de documentos cargados: {len(self.documents)}\")\n",
    "        \n",
    "        # Guardar estadísticas básicas\n",
    "        topics = set()\n",
    "        for doc in self.documents:\n",
    "            topics.update(doc['topics'])\n",
    "            \n",
    "        print(f\"Estadísticas del corpus:\")\n",
    "        print(f\"- Número de documentos: {len(self.documents)}\")\n",
    "        print(f\"- Número de tópicos únicos: {len(topics)}\")\n",
    "        print(f\"- Tópicos más comunes: {self.get_common_topics(5)}\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesa un texto individual\n",
    "        \"\"\"\n",
    "        # Convertir a minúsculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Eliminar caracteres especiales y números\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "        \n",
    "        # Tokenización\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Eliminar stop words y aplicar stemming\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens \n",
    "                 if token not in self.stop_words and len(token) > 2]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def process_documents(self):\n",
    "        \"\"\"\n",
    "        Preprocesa todos los documentos del corpus\n",
    "        \"\"\"\n",
    "        print(\"Preprocesando documentos...\")\n",
    "        for doc in tqdm(self.documents, desc=\"Preprocesando\"):\n",
    "            # Combinar título y cuerpo para el procesamiento\n",
    "            full_text = f\"{doc['title']} {doc['body']}\"\n",
    "            tokens = self.preprocess_text(full_text)\n",
    "            \n",
    "            self.processed_docs.append({\n",
    "                'id': doc['id'],\n",
    "                'tokens': tokens,\n",
    "                'original': doc\n",
    "            })\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"\n",
    "        Construye el índice invertido\n",
    "        \"\"\"\n",
    "        print(\"Construyendo índice invertido...\")\n",
    "        self.index = {}\n",
    "        \n",
    "        for doc_idx, doc in enumerate(tqdm(self.processed_docs, desc=\"Indexando\")):\n",
    "            for position, token in enumerate(doc['tokens']):\n",
    "                if token not in self.index:\n",
    "                    self.index[token] = []\n",
    "                self.index[token].append({\n",
    "                    'doc_id': doc['id'],\n",
    "                    'doc_idx': doc_idx,\n",
    "                    'position': position\n",
    "                })\n",
    "        \n",
    "        print(f\"Índice construido con {len(self.index)} términos\")\n",
    "\n",
    "    def get_common_topics(self, n=5):\n",
    "        \"\"\"\n",
    "        Obtiene los n tópicos más comunes\n",
    "        \"\"\"\n",
    "        topic_count = {}\n",
    "        for doc in self.documents:\n",
    "            for topic in doc['topics']:\n",
    "                topic_count[topic] = topic_count.get(topic, 0) + 1\n",
    "        \n",
    "        return sorted(topic_count.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    def create_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Crea la representación vectorial de los documentos\n",
    "        \"\"\"\n",
    "        print(\"Creando representación vectorial...\")\n",
    "        \n",
    "        # Crear corpus de documentos procesados\n",
    "        corpus = [' '.join(doc['tokens']) for doc in self.processed_docs]\n",
    "        \n",
    "        # Vectorización TF-IDF\n",
    "        self.vectorizer = TfidfVectorizer(lowercase=False)  # lowercase=False porque ya está preprocesado\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        print(f\"Matriz TF-IDF creada con forma: {self.tfidf_matrix.shape}\")\n",
    "\n",
    "    def search(self, query, top_k=10):\n",
    "        \"\"\"\n",
    "        Realiza una búsqueda en el corpus\n",
    "        Args:\n",
    "            query (str): Consulta del usuario\n",
    "            top_k (int): Número de resultados a retornar\n",
    "        Returns:\n",
    "            list: Lista de documentos más relevantes\n",
    "        \"\"\"\n",
    "        # Preprocesar la consulta\n",
    "        query_tokens = self.preprocess_text(query)\n",
    "        query_text = ' '.join(query_tokens)\n",
    "        \n",
    "        # Vectorizar la consulta\n",
    "        query_vec = self.vectorizer.transform([query_text])\n",
    "        \n",
    "        # Calcular similitud\n",
    "        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Obtener los documentos más relevantes\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc = self.processed_docs[idx]\n",
    "            original_doc = doc['original']\n",
    "            \n",
    "            # Calcular un extracto relevante\n",
    "            preview = original_doc['body'][:200] + '...' if len(original_doc['body']) > 200 else original_doc['body']\n",
    "            \n",
    "            results.append({\n",
    "                'id': doc['id'],\n",
    "                'title': original_doc['title'],\n",
    "                'preview': preview,\n",
    "                'topics': original_doc['topics'],\n",
    "                'similarity': float(similarities[idx]),\n",
    "                'file_name': original_doc['file_name']\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dcc55d9-8d23-4b99-8d89-732639609998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando corpus Reuters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos cargados: 0\n",
      "Estadísticas del corpus:\n",
      "- Número de documentos: 0\n",
      "- Número de tópicos únicos: 0\n",
      "- Tópicos más comunes: []\n",
      "Preprocesando documentos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocesando: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construyendo índice invertido...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexando: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice construido con 0 términos\n",
      "Creando representación vectorial...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m reuters_system\u001b[38;5;241m.\u001b[39mprocess_documents()\n\u001b[0;32m     11\u001b[0m reuters_system\u001b[38;5;241m.\u001b[39mbuild_index()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mreuters_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Ejemplo de búsqueda\u001b[39;00m\n\u001b[0;32m     15\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moil prices impact on global economy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 192\u001b[0m, in \u001b[0;36mReutersImplementation.create_vectorizer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Vectorización TF-IDF\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# lowercase=False porque ya está preprocesado\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatriz TF-IDF creada con forma: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfidf_matrix\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1293\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Ruta a los documentos Reuters\n",
    "    data_path = r\"C:\\Users\\USER\\Documents\\EPN\\SEPTIMO SEMESTRE\\RECUPERACIÓN_INFORMACIÓN\\IR-2024B\\data\\reuters\\training\"\n",
    "    \n",
    "    # Crear instancia del sistema\n",
    "    reuters_system = ReutersImplementation(data_path)\n",
    "    \n",
    "    # Cargar y procesar el corpus\n",
    "    reuters_system.load_corpus()\n",
    "    reuters_system.process_documents()\n",
    "    reuters_system.build_index()\n",
    "    reuters_system.create_vectorizer()\n",
    "    \n",
    "    # Ejemplo de búsqueda\n",
    "    query = \"oil prices impact on global economy\"\n",
    "    results = reuters_system.search(query)\n",
    "    \n",
    "    print(\"\\nResultados de búsqueda para:\", query)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {result['title']} (Score: {result['similarity']:.4f})\")\n",
    "        print(f\"ID: {result['id']} - Archivo: {result['file_name']}\")\n",
    "        print(f\"Tópicos: {', '.join(result['topics'])}\")\n",
    "        print(f\"Preview: {result['preview'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe50ab1-b336-45a0-8c7e-0b35ac605cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ReutersImplementation:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.documents = []\n",
    "        self.processed_docs = []\n",
    "        self.index = {}\n",
    "        self.vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        \n",
    "        # Inicializar NLTK\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            print(\"Descargando recursos NLTK necesarios...\")\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "    def load_single_file(self, file_path):\n",
    "        docs = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "                # Limpiar el contenido para manejar casos especiales de Reuters\n",
    "                content = content.replace('&', '&amp;')\n",
    "                \n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                reuters_docs = soup.find_all('reuters')\n",
    "                \n",
    "                for doc in reuters_docs:\n",
    "                    # Extraer campos relevantes\n",
    "                    doc_id = doc.get('newid', '')\n",
    "                    \n",
    "                    # Extraer texto\n",
    "                    text_tag = doc.find('text')\n",
    "                    if text_tag:\n",
    "                        # Obtener título\n",
    "                        title_tag = text_tag.find('title')\n",
    "                        title = title_tag.get_text().strip() if title_tag else ''\n",
    "                        \n",
    "                        # Obtener cuerpo\n",
    "                        body = ''\n",
    "                        if text_tag.body:\n",
    "                            body = text_tag.body.get_text().strip()\n",
    "                        else:\n",
    "                            # Si no hay tag body específico, tomar todo el contenido del text\n",
    "                            body = text_tag.get_text().strip()\n",
    "                            if title:  # Remover el título del cuerpo si existe\n",
    "                                body = body.replace(title, '', 1).strip()\n",
    "                        \n",
    "                        # Extraer tópicos\n",
    "                        topics_tag = doc.find('topics')\n",
    "                        topics = [t.get_text().strip() for t in doc.find_all('d')] if topics_tag else []\n",
    "                        \n",
    "                        if body:  # Solo incluir documentos que tengan contenido\n",
    "                            docs.append({\n",
    "                                'id': doc_id,\n",
    "                                'title': title,\n",
    "                                'body': body,\n",
    "                                'topics': topics,\n",
    "                                'file_name': os.path.basename(file_path)\n",
    "                            })\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando archivo {file_path}: {str(e)}\")\n",
    "            \n",
    "        return docs\n",
    "\n",
    "    def load_corpus(self):\n",
    "        print(\"Cargando corpus Reuters...\")\n",
    "        files = [f for f in os.listdir(self.data_path) if f.endswith('.sgm')]\n",
    "        \n",
    "        for file_name in tqdm(files, desc=\"Procesando archivos\"):\n",
    "            file_path = os.path.join(self.data_path, file_name)\n",
    "            docs = self.load_single_file(file_path)\n",
    "            self.documents.extend(docs)\n",
    "            \n",
    "        print(f\"Total de documentos cargados: {len(self.documents)}\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesa un texto individual con reglas menos agresivas\n",
    "        \"\"\"\n",
    "        # Convertir a minúsculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Reemplazar saltos de línea con espacios\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        \n",
    "        # Mantener números y algunos caracteres especiales importantes\n",
    "        text = re.sub(r'[^\\w\\s.-]', ' ', text)\n",
    "        \n",
    "        # Tokenización\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Lista personalizada de stop words (más permisiva)\n",
    "        custom_stops = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to'}\n",
    "        \n",
    "        # Filtrado menos agresivo\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            # Mantener números y términos con números\n",
    "            if token.isdigit() or any(c.isdigit() for c in token):\n",
    "                processed_tokens.append(token)\n",
    "                continue\n",
    "                \n",
    "            # Aplicar stemming solo a palabras que no son stop words\n",
    "            if token not in custom_stops and len(token) > 1:\n",
    "                stemmed = self.stemmer.stem(token)\n",
    "                processed_tokens.append(stemmed)\n",
    "        \n",
    "        return processed_tokens\n",
    "\n",
    "    def process_documents(self):\n",
    "        print(\"Preprocesando documentos...\")\n",
    "        self.processed_docs = []\n",
    "        \n",
    "        for doc in tqdm(self.documents, desc=\"Preprocesando\"):\n",
    "            # Procesar título y cuerpo por separado\n",
    "            title_tokens = self.preprocess_text(doc['title']) if doc['title'] else []\n",
    "            body_tokens = self.preprocess_text(doc['body']) if doc['body'] else []\n",
    "            \n",
    "            # Dar más peso a los tokens del título repitiéndolos\n",
    "            all_tokens = title_tokens * 2 + body_tokens\n",
    "            \n",
    "            if all_tokens:  # Solo agregar documentos que tengan tokens\n",
    "                self.processed_docs.append({\n",
    "                    'id': doc['id'],\n",
    "                    'tokens': all_tokens,\n",
    "                    'original': doc\n",
    "                })\n",
    "\n",
    "    def build_index(self):\n",
    "        print(\"Construyendo índice invertido...\")\n",
    "        self.index = {}\n",
    "        \n",
    "        for doc_idx, doc in enumerate(tqdm(self.processed_docs, desc=\"Indexando\")):\n",
    "            for position, token in enumerate(doc['tokens']):\n",
    "                if token not in self.index:\n",
    "                    self.index[token] = []\n",
    "                self.index[token].append({\n",
    "                    'doc_id': doc['id'],\n",
    "                    'doc_idx': doc_idx,\n",
    "                    'position': position\n",
    "                })\n",
    "        \n",
    "        print(f\"Índice construido con {len(self.index)} términos\")\n",
    "\n",
    "    def create_vectorizer(self):\n",
    "        print(\"Creando representación vectorial...\")\n",
    "        \n",
    "        # Crear corpus de documentos procesados\n",
    "        corpus = [' '.join(doc['tokens']) for doc in self.processed_docs]\n",
    "        \n",
    "        # Configurar TF-IDF con parámetros más permisivos\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            lowercase=False,  # Ya está en minúsculas\n",
    "            token_pattern=r'(?u)\\b\\w+\\b',  # Patrón más permisivo\n",
    "            min_df=2,  # Aparecer al menos en 2 documentos\n",
    "            max_df=0.95  # Ignorar términos que aparecen en más del 95% de los documentos\n",
    "        )\n",
    "        \n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(corpus)\n",
    "        print(f\"Matriz TF-IDF creada con forma: {self.tfidf_matrix.shape}\")\n",
    "\n",
    "    def search(self, query, top_k=10):\n",
    "        # Preprocesar la consulta\n",
    "        query_tokens = self.preprocess_text(query)\n",
    "        query_text = ' '.join(query_tokens)\n",
    "        \n",
    "        # Vectorizar la consulta\n",
    "        query_vec = self.vectorizer.transform([query_text])\n",
    "        \n",
    "        # Calcular similitud\n",
    "        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Obtener los documentos más relevantes\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc = self.processed_docs[idx]\n",
    "            original_doc = doc['original']\n",
    "            \n",
    "            # Calcular un extracto relevante\n",
    "            preview = original_doc['body'][:200] + '...' if len(original_doc['body']) > 200 else original_doc['body']\n",
    "            \n",
    "            results.append({\n",
    "                'id': doc['id'],\n",
    "                'title': original_doc['title'],\n",
    "                'preview': preview,\n",
    "                'topics': original_doc['topics'],\n",
    "                'similarity': float(similarities[idx]),\n",
    "                'file_name': original_doc['file_name']\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffdd1a6b-c9a8-4845-af61-6f47273cbb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando corpus Reuters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos cargados: 0\n",
      "Preprocesando documentos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocesando: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construyendo índice invertido...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexando: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice construido con 0 términos\n",
      "Creando representación vectorial...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m reuters_system\u001b[38;5;241m.\u001b[39mprocess_documents()\n\u001b[0;32m     12\u001b[0m reuters_system\u001b[38;5;241m.\u001b[39mbuild_index()\n\u001b[1;32m---> 13\u001b[0m \u001b[43mreuters_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Realizar una búsqueda de ejemplo\u001b[39;00m\n\u001b[0;32m     16\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moil prices impact on global economy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 179\u001b[0m, in \u001b[0;36mReutersImplementation.create_vectorizer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Configurar TF-IDF con parámetros más permisivos\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[0;32m    173\u001b[0m     lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Ya está en minúsculas\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?u)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Patrón más permisivo\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# Aparecer al menos en 2 documentos\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m  \u001b[38;5;66;03m# Ignorar términos que aparecen en más del 95% de los documentos\u001b[39;00m\n\u001b[0;32m    177\u001b[0m )\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatriz TF-IDF creada con forma: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfidf_matrix\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1293\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Ruta a los documentos Reuters\n",
    "    data_path = r\"..\\data\\reuters\\training\"\n",
    "    \n",
    "    # Crear y configurar el sistema\n",
    "    reuters_system = ReutersImplementation(data_path)\n",
    "    \n",
    "    # Procesar el corpus\n",
    "    reuters_system.load_corpus()\n",
    "    reuters_system.process_documents()\n",
    "    reuters_system.build_index()\n",
    "    reuters_system.create_vectorizer()\n",
    "    \n",
    "    # Realizar una búsqueda de ejemplo\n",
    "    query = \"oil prices impact on global economy\"\n",
    "    results = reuters_system.search(query)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\nResultados para la búsqueda: '{query}'\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {result['title']} (Score: {result['similarity']:.4f})\")\n",
    "        print(f\"ID: {result['id']} - Archivo: {result['file_name']}\")\n",
    "        print(f\"Tópicos: {', '.join(result['topics'])}\")\n",
    "        print(f\"Preview: {result['preview'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb228c9d-6447-4404-8cd0-c8525a6f92f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando corpus Reuters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos cargados: 0\n",
      "Preprocesando documentos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocesando: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construyendo índice invertido...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexando: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice construido con 0 términos\n",
      "Creando representación vectorial...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m reuters_system\u001b[38;5;241m.\u001b[39mprocess_documents()\n\u001b[0;32m      5\u001b[0m reuters_system\u001b[38;5;241m.\u001b[39mbuild_index()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mreuters_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Realizar búsqueda\u001b[39;00m\n\u001b[0;32m      9\u001b[0m results \u001b[38;5;241m=\u001b[39m reuters_system\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moil prices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 179\u001b[0m, in \u001b[0;36mReutersImplementation.create_vectorizer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Configurar TF-IDF con parámetros más permisivos\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[0;32m    173\u001b[0m     lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Ya está en minúsculas\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?u)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Patrón más permisivo\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# Aparecer al menos en 2 documentos\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m  \u001b[38;5;66;03m# Ignorar términos que aparecen en más del 95% de los documentos\u001b[39;00m\n\u001b[0;32m    177\u001b[0m )\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatriz TF-IDF creada con forma: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfidf_matrix\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1293\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Configurar y cargar el sistema\n",
    "reuters_system = ReutersImplementation(data_path)\n",
    "reuters_system.load_corpus()\n",
    "reuters_system.process_documents()\n",
    "reuters_system.build_index()\n",
    "reuters_system.create_vectorizer()\n",
    "\n",
    "# Realizar búsqueda\n",
    "results = reuters_system.search(\"oil prices\")\n",
    "\n",
    "# Mostrar resultados\n",
    "for result in results:\n",
    "    print(f\"{result['title']} - Score: {result['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225010cf-9659-4280-93aa-479ccb413d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
